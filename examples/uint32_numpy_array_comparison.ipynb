{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ChiniDataset vs MosaicML — uint32 Tokenized Write + Read\n",
        "\n",
        "1. Setup\n",
        "2. Tokenize + Write (inline)\n",
        "3. Read\n",
        "4. Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup\n",
        "\n",
        "Download Wikipedia EN shard (156k articles) and build a simple word-level tokenizer (O(1) dict lookup)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!uv pip install git+https://github.com/Scicom-AI-Enterprise-Organization/ChiniDataset.git\n",
        "!uv pip install mosaicml-streaming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pyarrow.parquet as pq\n",
        "import shutil\n",
        "import time\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "print(\"Downloading Wikipedia EN shard...\")\n",
        "parquet_path = hf_hub_download(\n",
        "    repo_id=\"wikimedia/wikipedia\",\n",
        "    filename=\"20231101.en/train-00000-of-00041.parquet\",\n",
        "    repo_type=\"dataset\",\n",
        ")\n",
        "\n",
        "table = pq.read_table(parquet_path)\n",
        "texts = table[\"text\"].to_pylist()\n",
        "N = len(texts)\n",
        "print(f\"Loaded {N:,} articles\")\n",
        "\n",
        "# Build word-level vocab — O(1) dict lookup per word\n",
        "print(\"Building vocab...\")\n",
        "vocab = {}\n",
        "for t in tqdm(texts, desc=\"Building vocab\"):\n",
        "    for word in t.split():\n",
        "        if word not in vocab:\n",
        "            vocab[word] = len(vocab)\n",
        "print(f\"Vocab size: {len(vocab):,} words\")\n",
        "\n",
        "def tokenize(text):\n",
        "    return np.array([vocab.get(w, 0) for w in text.split()], dtype=np.uint32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Tokenize + Write (inline)\n",
        "\n",
        "Tokenize each article and write the uint32 token arrays in the same loop — no separate tokenization pass."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from chinidataset import ParquetWriter\n",
        "\n",
        "chini_out = \"./bench_chinidataset\"\n",
        "if Path(chini_out).exists():\n",
        "    shutil.rmtree(chini_out)\n",
        "\n",
        "t0 = time.perf_counter()\n",
        "\n",
        "with ParquetWriter(out=chini_out, columns={\"input_ids\": \"uint32[]\", \"labels\": \"uint32[]\"}) as w:\n",
        "    for t in tqdm(texts, desc=\"ChiniDataset\"):\n",
        "        tokens = tokenize(t)\n",
        "        w.write({\"input_ids\": tokens, \"labels\": tokens})\n",
        "        \n",
        "chini_write = time.perf_counter() - t0\n",
        "print(f\"ChiniDataset: {chini_write:.2f}s | {N / chini_write:,.0f} rows/s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from streaming import MDSWriter\n",
        "from streaming.base.format.mds.encodings import Encoding, _encodings\n",
        "\n",
        "class UInt32(Encoding):\n",
        "    def encode(self, obj):\n",
        "        return obj.tobytes()\n",
        "    def decode(self, data):\n",
        "        return np.frombuffer(data, np.uint32)\n",
        "\n",
        "_encodings[\"uint32\"] = UInt32\n",
        "\n",
        "mds_out = \"./bench_mosaicml\"\n",
        "if Path(mds_out).exists():\n",
        "    shutil.rmtree(mds_out)\n",
        "\n",
        "t0 = time.perf_counter()\n",
        "\n",
        "with MDSWriter(out=mds_out, columns={\"input_ids\": \"uint32\", \"labels\": \"uint32\"}) as w:\n",
        "    for t in tqdm(texts, desc=\"MosaicML\"):\n",
        "        tokens = tokenize(t)\n",
        "        w.write({\"input_ids\": tokens, \"labels\": tokens})\n",
        "        \n",
        "mds_write = time.perf_counter() - t0\n",
        "print(f\"MosaicML: {mds_write:.2f}s | {N / mds_write:,.0f} rows/s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Read\n",
        "\n",
        "Read all rows back and measure throughput."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from chinidataset import StreamingDataset\n",
        "\n",
        "ds = StreamingDataset(local=chini_out)\n",
        "\n",
        "t0 = time.perf_counter()\n",
        "count = 0\n",
        "for sample in tqdm(ds, desc=\"ChiniDataset read\", total=N):\n",
        "    _ = sample[\"input_ids\"]\n",
        "    count += 1\n",
        "\n",
        "chini_read = time.perf_counter() - t0\n",
        "print(f\"ChiniDataset: {count:,} rows | {chini_read:.2f}s | {count / chini_read:,.0f} rows/s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from streaming import StreamingDataset as MosaicDS\n",
        "\n",
        "mds_ds = MosaicDS(local=mds_out, shuffle=False, batch_size=1)\n",
        "\n",
        "t0 = time.perf_counter()\n",
        "count = 0\n",
        "for sample in tqdm(mds_ds, desc=\"MosaicML read\", total=N):\n",
        "    _ = sample[\"input_ids\"]\n",
        "    count += 1\n",
        "\n",
        "mds_read = time.perf_counter() - t0\n",
        "print(f\"MosaicML: {count:,} rows | {mds_read:.2f}s | {count / mds_read:,.0f} rows/s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Dataset: Wikipedia EN ({N:,} articles), simple word tokenizer, uint32\")\n",
        "print()\n",
        "print(f\"{'Metric':<25} {'MosaicML':>15} {'ChiniDataset':>15} {'Speedup':>10}\")\n",
        "print(\"-\" * 65)\n",
        "print(f\"{'Tokenize+Write (rows/s)':<25} {N / mds_write:>12,.0f}/s {N / chini_write:>12,.0f}/s {mds_write / chini_write:>9.1f}x\")\n",
        "print(f\"{'Read (rows/s)':<25} {count / mds_read:>12,.0f}/s {count / chini_read:>12,.0f}/s {mds_read / chini_read:>9.1f}x\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
