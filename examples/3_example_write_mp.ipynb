{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# `write_mp` — Parallel Write from HuggingFace Dataset\n",
        "\n",
        "Load a multi-shard HuggingFace dataset, tokenize with a transform function, write in parallel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!uv pip install git+https://github.com/Scicom-AI-Enterprise-Organization/ChiniDataset.git\n",
        "!uv pip install datasets transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load dataset\n",
        "\n",
        "Wikipedia EN has 41 parquet shards. `load_dataset` handles all of them — returns one indexable dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"wikimedia/wikipedia\", \"20231101.en\", split=\"train\")\n",
        "print(f\"Loaded {len(ds):,} articles\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Define transform\n",
        "\n",
        "The transform runs inside each worker — this is where you do per-row processing.\n",
        "\n",
        "Must be a **top-level function** (not a lambda) for multiprocessing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "def transform(row):\n",
        "    # preprocess text\n",
        "    text = row[\"text\"].strip().lower()\n",
        "    text = f\"title: {row['title']}\\n\\n{text}\"\n",
        "\n",
        "    # tokenize\n",
        "    ids = tokenizer(text, add_special_tokens=False)[\"input_ids\"]\n",
        "\n",
        "    return {\"input_ids\": np.array(ids, dtype=np.uint32)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. `write_mp`\n",
        "\n",
        "Partitions the dataset across 4 workers. Each worker iterates its chunk, applies `transform`, and writes to its own subdirectory. Index files are merged automatically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from chinidataset import ParquetWriter\n",
        "\n",
        "OUT = \"./wiki_tokenized\"\n",
        "columns = {\"input_ids\": \"uint32[]\"}\n",
        "\n",
        "with ParquetWriter(out=OUT, columns=columns, exist_ok=True) as writer:\n",
        "    writer.write_mp(ds, num_workers=4, transform=transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Verify"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json, os\n",
        "\n",
        "with open(os.path.join(OUT, \"index.json\")) as f:\n",
        "    idx = json.load(f)\n",
        "\n",
        "total = sum(s[\"samples\"] for s in idx[\"shards\"])\n",
        "shards = len(idx[\"shards\"])\n",
        "partitions = len(set(s[\"raw_data\"][\"basename\"].split(\"/\")[0] for s in idx[\"shards\"]))\n",
        "\n",
        "print(f\"Total samples: {total:,}\")\n",
        "print(f\"Shards: {shards}\")\n",
        "print(f\"Partitions: {partitions}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
