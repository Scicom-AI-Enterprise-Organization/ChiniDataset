{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LRU Cache - Look-Ahead"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2mUsing Python 3.11.14 environment at: /Users/ariff.a/Documents/GitHub/StreamingDataset/.venv\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m22 packages\u001b[0m \u001b[2min 46ms\u001b[0m\u001b[0m                                         \u001b[0m\n",
            "\u001b[2K   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m chinidataset\u001b[2m @ file:///Users/ariff.a/Documents/GitHub/StreamingDatas\u001b[0m\n",
            "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m chinidataset\u001b[2m @ file:///Users/ariff.a/Documents/GitHub/StreamingDatas\u001b[0m\n",
            "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m chinidataset\u001b[2m @ file:///Users/ariff.a/Documents/GitHub/StreamingDatas\u001b[0m\n",
            "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m chinidataset\u001b[2m @ file:///Users/ariff.a/Documents/GitHub/StreamingDatas\u001b[0m\n",
            "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m chinidataset\u001b[2m @ file:///Users/ariff.a/Documents/GitHub/StreamingDatas\u001b[0m\n",
            "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m chinidataset\u001b[2m @ file:///Users/ariff.a/Documents/GitHub/StreamingDatas\u001b[0m\n",
            "\u001b[2K\u001b[1A      \u001b[32m\u001b[1mBuilt\u001b[0m\u001b[39m chinidataset\u001b[2m @ file:///Users/ariff.a/Documents/GitHub/StreamingDatas\u001b[0m\n",
            "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 1.04s\u001b[0m\u001b[0m                                              \n",
            "\u001b[2mUninstalled \u001b[1m1 package\u001b[0m \u001b[2min 3ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 4ms\u001b[0m\u001b[0m (from file:///Users/ariff.a/Docum\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mchinidataset\u001b[0m\u001b[2m==0.1.0 (from git+https://github.com/Scicom-AI-Enterprise-Organization/ChiniDataset.git@33f269ec8713a1beee557e457fecb076f7bbfb30)\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mchinidataset\u001b[0m\u001b[2m==0.2.0 (from file:///Users/ariff.a/Documents/GitHub/StreamingDataset)\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!uv pip install -e ../\n",
        "# !uv pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/ariff.a/Documents/GitHub/StreamingDataset/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading Wikipedia EN shard 0 ...\n",
            "  156,289 articles\n",
            "Building word vocabulary ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  counting words: 100%|██████████| 156289/156289 [00:14<00:00, 10909.57it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  50,000 words in vocab\n"
          ]
        }
      ],
      "source": [
        "## 1. Load Wikipedia & build word vocabulary\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "PARQUET_URL = \"hf://datasets/wikimedia/wikipedia/20231101.en/train-00000-of-00041.parquet\"\n",
        "\n",
        "print(\"Loading Wikipedia EN shard 0 ...\")\n",
        "wiki = load_dataset(\"parquet\", data_files=PARQUET_URL, split=\"train\")\n",
        "print(f\"  {len(wiki):,} articles\")\n",
        "\n",
        "print(\"Building word vocabulary ...\")\n",
        "counter = Counter()\n",
        "for row in tqdm(wiki, desc=\"  counting words\"):\n",
        "    counter.update(row[\"text\"].split())\n",
        "\n",
        "UNK_ID = 0\n",
        "VOCAB = {\"<unk>\": UNK_ID}\n",
        "for i, (w, _) in enumerate(counter.most_common(49_999), 1):\n",
        "    VOCAB[w] = i\n",
        "print(f\"  {len(VOCAB):,} words in vocab\")\n",
        "\n",
        "def tokenize(text):\n",
        "    return np.array([VOCAB.get(w, UNK_ID) for w in text.split()], dtype=np.uint32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Directory /Users/ariff.a/Documents/GitHub/StreamingDataset/examples/demo_look_ahead_data exists; removing contents.\n",
            "Writing: 100%|██████████| 156289/156289 [00:14<00:00, 11106.75it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done! 156,289 samples across 14 shards\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "## 2. Write to ChiniDataset shards\n",
        "\n",
        "from chinidataset import ParquetWriter\n",
        "\n",
        "DATA_DIR = Path(\"./demo_look_ahead_data\")\n",
        "\n",
        "columns = {\"input_ids\": \"uint32[]\", \"labels\": \"uint32[]\"}\n",
        "\n",
        "with ParquetWriter(out=str(DATA_DIR), columns=columns, exist_ok=True) as w:\n",
        "    for row in tqdm(wiki, desc=\"Writing\"):\n",
        "        ids = tokenize(row[\"text\"])\n",
        "        w.write({\"input_ids\": ids, \"labels\": ids})\n",
        "\n",
        "n_shards = len(list(DATA_DIR.glob(\"shard.*\")))\n",
        "print(f\"Done! {len(wiki):,} samples across {n_shards} shards\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  look_ahead=0  warmup: 156,289 samples in 1.104s (141,517 samples/s)\n",
            "  look_ahead=0  run 1: 156,289 samples in 0.811s (192,691 samples/s)\n",
            "  look_ahead=0  run 2: 156,289 samples in 0.749s (208,748 samples/s)\n",
            "  look_ahead=0  run 3: 156,289 samples in 0.753s (207,629 samples/s)\n",
            "\n",
            "  look_ahead=2  warmup: 156,289 samples in 0.566s (276,081 samples/s)\n",
            "  look_ahead=2  run 1: 156,289 samples in 0.567s (275,476 samples/s)\n",
            "  look_ahead=2  run 2: 156,289 samples in 0.560s (278,867 samples/s)\n",
            "  look_ahead=2  run 3: 156,289 samples in 0.588s (265,857 samples/s)\n",
            "\n",
            "  look_ahead=4  warmup: 156,289 samples in 0.564s (277,271 samples/s)\n",
            "  look_ahead=4  run 1: 156,289 samples in 0.589s (265,534 samples/s)\n",
            "  look_ahead=4  run 2: 156,289 samples in 0.593s (263,701 samples/s)\n",
            "  look_ahead=4  run 3: 156,289 samples in 0.669s (233,614 samples/s)\n",
            "\n",
            "============================================================\n",
            "    look_ahead    Avg time    Avg samp/s   Speedup\n",
            "  --------------------------------------------------\n",
            "             0      0.771s      202,752/s     1.00x\n",
            "             2      0.572s      273,288/s     1.35x\n",
            "             4      0.617s      253,405/s     1.25x\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "## 3. Benchmark: `look_ahead=0` vs `look_ahead=2`\n",
        "from chinidataset import StreamingDataset\n",
        "import gc\n",
        "\n",
        "REPEATS = 4  # 1 warmup + 3 measured\n",
        "results = {}\n",
        "\n",
        "for la in [0, 2, 4]:\n",
        "    times = []\n",
        "    for run in range(REPEATS):\n",
        "        ds = StreamingDataset(\n",
        "            local=str(DATA_DIR),\n",
        "            look_ahead=la,\n",
        "            max_open_shards=max(8, la + 2),\n",
        "            shuffle=False,\n",
        "        )\n",
        "\n",
        "        t0 = time.perf_counter()\n",
        "        count = 0\n",
        "        for sample in ds:\n",
        "            _ = sample[\"input_ids\"]\n",
        "            count += 1\n",
        "        elapsed = time.perf_counter() - t0\n",
        "\n",
        "        label = \"warmup\" if run == 0 else f\"run {run}\"\n",
        "        print(f\"  look_ahead={la}  {label}: {count:,} samples in {elapsed:.3f}s \"\n",
        "              f\"({count / elapsed:,.0f} samples/s)\")\n",
        "        times.append(elapsed)\n",
        "\n",
        "        del ds\n",
        "        gc.collect()\n",
        "\n",
        "    measured = times[1:]  # skip warmup\n",
        "    results[la] = {\n",
        "        \"avg\": np.mean(measured),\n",
        "        \"best\": min(measured),\n",
        "        \"samples\": count,\n",
        "    }\n",
        "    print()\n",
        "\n",
        "# Summary\n",
        "n = results[0][\"samples\"]\n",
        "print(\"=\" * 60)\n",
        "print(f\"  {'look_ahead':>12s}  {'Avg time':>10s}  {'Avg samp/s':>12s}  {'Speedup':>8s}\")\n",
        "print(f\"  {'-' * 50}\")\n",
        "for la in [0, 2, 4]:\n",
        "    r = results[la]\n",
        "    sp = results[0][\"avg\"] / r[\"avg\"]\n",
        "    print(f\"  {la:>12d}  {r['avg']:>9.3f}s  {n / r['avg']:>11,.0f}/s  {sp:>7.2f}x\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accessed shard  0 -> cache: [0]  (size=1)\n",
            "Accessed shard  1 -> cache: [0, 1]  (size=2)\n",
            "Accessed shard  2 -> cache: [0, 1, 2]  (size=3)\n",
            "Accessed shard  3 -> cache: [1, 2, 3]  (size=3)\n",
            "Accessed shard  4 -> cache: [2, 3, 4]  (size=3)\n",
            "Accessed shard  5 -> cache: [3, 4, 5]  (size=3)\n",
            "Accessed shard  6 -> cache: [4, 5, 6]  (size=3)\n",
            "Accessed shard  7 -> cache: [5, 6, 7]  (size=3)\n",
            "Accessed shard  8 -> cache: [6, 7, 8]  (size=3)\n",
            "Accessed shard  9 -> cache: [7, 8, 9]  (size=3)\n",
            "Accessed shard 10 -> cache: [8, 9, 10]  (size=3)\n",
            "Accessed shard 11 -> cache: [9, 10, 11]  (size=3)\n",
            "Accessed shard 12 -> cache: [10, 11, 12]  (size=3)\n",
            "Accessed shard 13 -> cache: [11, 12, 13]  (size=3)\n",
            "\n",
            "max_open_shards=3, so only 3 most recent shards stay in memory.\n",
            "Oldest shard is evicted each time a new one is loaded.\n"
          ]
        }
      ],
      "source": [
        "## 4. Inspect LRU cache behavior\n",
        "\n",
        "ds = StreamingDataset(\n",
        "    local=str(DATA_DIR),\n",
        "    look_ahead=0,          # disable look-ahead so we can see pure LRU\n",
        "    max_open_shards=3,     # small cache to trigger evictions\n",
        "    shuffle=False,\n",
        ")\n",
        "\n",
        "samples_per_shard = ds._samples_per_shard\n",
        "\n",
        "# Access first sample of each shard\n",
        "offset = 0\n",
        "for shard_idx in range(ds.num_shards):\n",
        "    _ = ds[offset]\n",
        "    cached = list(ds._readers.keys())\n",
        "    print(f\"Accessed shard {shard_idx:>2d} -> cache: {cached}  (size={len(cached)})\")\n",
        "    offset += samples_per_shard[shard_idx]\n",
        "\n",
        "print(f\"\\nmax_open_shards=3, so only 3 most recent shards stay in memory.\")\n",
        "print(f\"Oldest shard is evicted each time a new one is loaded.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After loading 0,1,2:  cache = [0, 1, 2]\n",
            "After touching 0:     cache = [1, 2, 0]\n",
            "After loading 3:      cache = [2, 0, 3]\n",
            "\n",
            "Shard 1 was evicted, not shard 0, because we touched 0 -- LRU in action!\n"
          ]
        }
      ],
      "source": [
        "## 5. LRU touch: re-access prevents eviction\n",
        "\n",
        "ds = StreamingDataset(\n",
        "    local=str(DATA_DIR),\n",
        "    look_ahead=0,\n",
        "    max_open_shards=3,\n",
        "    shuffle=False,\n",
        ")\n",
        "\n",
        "sps = ds._samples_per_shard\n",
        "\n",
        "# Load shards 0, 1, 2\n",
        "_ = ds[0]                       # shard 0\n",
        "_ = ds[sps[0]]                  # shard 1\n",
        "_ = ds[sps[0] + sps[1]]         # shard 2\n",
        "print(f\"After loading 0,1,2:  cache = {list(ds._readers.keys())}\")\n",
        "\n",
        "# Touch shard 0 again (moves to most recent)\n",
        "_ = ds[5]                       # shard 0 again\n",
        "print(f\"After touching 0:     cache = {list(ds._readers.keys())}\")\n",
        "\n",
        "# Load shard 3 — shard 1 gets evicted (not 0!)\n",
        "_ = ds[sps[0] + sps[1] + sps[2]]  # shard 3\n",
        "print(f\"After loading 3:      cache = {list(ds._readers.keys())}\")\n",
        "print(f\"\\nShard 1 was evicted, not shard 0, because we touched 0 -- LRU in action!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cleaned up demo data.\n"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "shutil.rmtree(DATA_DIR, ignore_errors=True)\n",
        "print(\"Cleaned up demo data.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
