{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ChiniDataset Example\n",
        "\n",
        "1. Write from HuggingFace dataset\n",
        "2. PyTorch DataLoader\n",
        "3. Custom DataLoader\n",
        "4. Inspect with pandas\n",
        "5. Writing NumPy uint32 arrays\n",
        "6. Multipacking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!uv pip install git+https://github.com/Scicom-AI-Enterprise-Organization/ChiniDataset.git\n",
        "!uv pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Write from HuggingFace Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import shutil\n",
        "from pathlib import Path\n",
        "from datasets import load_dataset\n",
        "from chinidataset import ParquetWriter\n",
        "\n",
        "hf_ds = load_dataset(\"stanfordnlp/imdb\", split=\"test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "OUT_DIR = \"./dataset\"\n",
        "if Path(OUT_DIR).exists():\n",
        "    shutil.rmtree(OUT_DIR)\n",
        "\n",
        "columns = {\"text\": \"str\", \"label\": \"int32\"}\n",
        "\n",
        "with ParquetWriter(out=OUT_DIR, columns=columns) as writer:\n",
        "    for row in hf_ds:\n",
        "        writer.write(row)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. PyTorch DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from chinidataset import StreamingDataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "ds = StreamingDataset(local=OUT_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "loader = DataLoader(ds, batch_size=32)\n",
        "\n",
        "for batch in loader:\n",
        "    text = batch[\"text\"]\n",
        "    label = batch[\"label\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Custom DataLoader\n",
        "\n",
        "1. **`DatasetFixed`** — wraps `StreamingDataset` in a map-style `Dataset`, does per-sample preprocessing in `__getitem__`\n",
        "2. **Custom `collator`** — batches text + labels (in a real training script, you'd tokenize here)\n",
        "3. **`DataLoader`** — wires it together with `collate_fn=collator`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from chinidataset import StreamingDataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "# 1. Dataset wrapper (same pattern as DatasetFixed in qwen3_adamw.py)\n",
        "\n",
        "class DatasetFixed(torch.utils.data.Dataset):\n",
        "    def __init__(self, local):\n",
        "        self.dataset = StreamingDataset(local=local)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        data = self.dataset[idx]\n",
        "        # Per-sample preprocessing:\n",
        "        # - cast dtypes, drop columns, filter, etc.\n",
        "        # In qwen3_adamw.py this casts to int64 and drops audio/text columns.\n",
        "        return data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "\n",
        "# 2. Custom collator\n",
        "#    cu_seq_lens for flash attention (packed training).\n",
        "#    Here we show the same structural pattern with text + labels.\n",
        "\n",
        "def collator(batch):\n",
        "    batch = [b for b in batch if b is not None]\n",
        "    texts = [b['text'] for b in batch]\n",
        "    labels = torch.tensor([b['label'] for b in batch], dtype=torch.long)\n",
        "\n",
        "    # In a real training script you'd tokenize here:\n",
        "    #   encoded = tokenizer(texts, padding=True, return_tensors='pt')\n",
        "    #   return {'input_ids': encoded.input_ids, 'labels': labels, ...}\n",
        "\n",
        "    return {\n",
        "        'text': texts,\n",
        "        'labels': labels,\n",
        "        'num_items_in_batch': torch.tensor(len(batch)),\n",
        "    }\n",
        "\n",
        "\n",
        "# 3. Wire it up\n",
        "dataset = DatasetFixed(OUT_DIR)\n",
        "print(f'Dataset: {len(dataset)} samples')\n",
        "print(f'Sample: {dataset[0]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Read Parquet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_parquet(f\"{OUT_DIR}/shard.00000.parquet\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Writing numpy uint32 arrays\n",
        "\n",
        "When preparing tokenized data for LLM training, token IDs are stored as numpy arrays of `uint32`. Why `uint32`?\n",
        "\n",
        "- Vocabulary sizes (e.g. 32k-150k tokens) fit well within uint32 range (0 to 4.2 billion)\n",
        "- Half the memory of int64 — matters when you have billions of tokens on disk\n",
        "- MosaicML's streaming uses a custom `UInt32` encoding class to handle this. With ChiniDataset, you just declare `uint32[]` in the column schema and pass numpy arrays directly — no custom encoding needed.\n",
        "\n",
        "`ParquetWriter` supports array columns via the `[]` suffix: `uint32[]`, `int64[]`, `float32[]`, etc. Each row can have a different-length array (variable-length sequences)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from transformers import AutoTokenizer\n",
        "from chinidataset import ParquetWriter\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "TOKENIZED_DIR = \"./tokenized_dataset\"\n",
        "if Path(TOKENIZED_DIR).exists():\n",
        "    shutil.rmtree(TOKENIZED_DIR)\n",
        "\n",
        "# uint32[] = variable-length array of uint32 per row\n",
        "# each row can have a different number of tokens\n",
        "columns = {\n",
        "    \"input_ids\": \"uint32[]\",\n",
        "    \"labels\": \"uint32[]\",\n",
        "}\n",
        "\n",
        "with ParquetWriter(out=TOKENIZED_DIR, columns=columns) as writer:\n",
        "    for row in hf_ds:\n",
        "        # tokenize text -> list of ints -> numpy uint32 array\n",
        "        tokens = tokenizer(row[\"text\"], add_special_tokens=False)[\"input_ids\"]\n",
        "        input_ids = np.array(tokens, dtype=np.uint32)\n",
        "\n",
        "        # for causal LM, labels = input_ids (predict next token)\n",
        "        writer.write({\n",
        "            \"input_ids\": input_ids,\n",
        "            \"labels\": input_ids,\n",
        "        })\n",
        "\n",
        "print(f\"Wrote {len(hf_ds)} rows to {TOKENIZED_DIR}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
