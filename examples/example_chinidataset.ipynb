{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ChiniDataset Example\n",
        "\n",
        "1. Write from HuggingFace dataset\n",
        "2. PyTorch DataLoader\n",
        "3. Custom DataLoader\n",
        "4. Inspect with pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!uv pip install git+https://github.com/Scicom-AI-Enterprise-Organization/ChiniDataset.git\n",
        "!uv pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Write from HuggingFace Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import shutil\n",
        "from pathlib import Path\n",
        "from datasets import load_dataset\n",
        "from chinidataset import ParquetWriter\n",
        "\n",
        "hf_ds = load_dataset(\"stanfordnlp/imdb\", split=\"test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "OUT_DIR = \"./dataset\"\n",
        "if Path(OUT_DIR).exists():\n",
        "    shutil.rmtree(OUT_DIR)\n",
        "\n",
        "columns = {\"text\": \"str\", \"label\": \"int32\"}\n",
        "\n",
        "with ParquetWriter(out=OUT_DIR, columns=columns) as writer:\n",
        "    for row in hf_ds:\n",
        "        writer.write(row)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. PyTorch DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from chinidataset import StreamingDataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "ds = StreamingDataset(local=OUT_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "loader = DataLoader(ds, batch_size=32)\n",
        "\n",
        "for batch in loader:\n",
        "    text = batch[\"text\"]\n",
        "    label = batch[\"label\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Custom DataLoader\n",
        "\n",
        "1. **`DatasetFixed`** — wraps `StreamingDataset` in a map-style `Dataset`, does per-sample preprocessing in `__getitem__`\n",
        "2. **Custom `collator`** — batches text + labels (in a real training script, you'd tokenize here)\n",
        "3. **`DataLoader`** — wires it together with `collate_fn=collator`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from chinidataset import StreamingDataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "# 1. Dataset wrapper (same pattern as DatasetFixed in qwen3_adamw.py)\n",
        "\n",
        "class DatasetFixed(torch.utils.data.Dataset):\n",
        "    def __init__(self, local):\n",
        "        self.dataset = StreamingDataset(local=local)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        data = self.dataset[idx]\n",
        "        # Per-sample preprocessing:\n",
        "        # - cast dtypes, drop columns, filter, etc.\n",
        "        # In qwen3_adamw.py this casts to int64 and drops audio/text columns.\n",
        "        return data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "\n",
        "# 2. Custom collator\n",
        "#    cu_seq_lens for flash attention (packed training).\n",
        "#    Here we show the same structural pattern with text + labels.\n",
        "\n",
        "def collator(batch):\n",
        "    batch = [b for b in batch if b is not None]\n",
        "    texts = [b['text'] for b in batch]\n",
        "    labels = torch.tensor([b['label'] for b in batch], dtype=torch.long)\n",
        "\n",
        "    # In a real training script you'd tokenize here:\n",
        "    #   encoded = tokenizer(texts, padding=True, return_tensors='pt')\n",
        "    #   return {'input_ids': encoded.input_ids, 'labels': labels, ...}\n",
        "\n",
        "    return {\n",
        "        'text': texts,\n",
        "        'labels': labels,\n",
        "        'num_items_in_batch': torch.tensor(len(batch)),\n",
        "    }\n",
        "\n",
        "\n",
        "# 3. Wire it up\n",
        "dataset = DatasetFixed(OUT_DIR)\n",
        "print(f'Dataset: {len(dataset)} samples')\n",
        "print(f'Sample: {dataset[0]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Read Parquet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_parquet(f\"{OUT_DIR}/shard.00000.parquet\")\n",
        "df.head()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
