{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ChiniDataset — uint32 Tokenized Write + Read\n",
        "\n",
        "1. Setup\n",
        "2. Tokenize + Write\n",
        "3. Read"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup\n",
        "\n",
        "Download Wikipedia EN shard (156k articles) and load GPT-2 tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!uv pip install git+https://github.com/Scicom-AI-Enterprise-Organization/ChiniDataset.git\n",
        "!uv pip install datasets transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "ds = load_dataset(\n",
        "    \"parquet\",\n",
        "    data_files=\"hf://datasets/wikimedia/wikipedia/20231101.en/train-00000-of-00041.parquet\",\n",
        "    split=\"train\",\n",
        ")\n",
        "N = len(ds)\n",
        "print(f\"Loaded {N:,} articles\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "def tokenize(text):\n",
        "    ids = tokenizer(text, add_special_tokens=False)[\"input_ids\"]\n",
        "    return np.array(ids, dtype=np.uint32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Tokenize + Write (inline)\n",
        "\n",
        "Tokenize each article and write the uint32 token arrays in the same loop — no separate tokenization pass."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from chinidataset import ParquetWriter\n",
        "\n",
        "chini_out = \"./bench_chinidataset\"\n",
        "\n",
        "col = {\"input_ids\": \"uint32[]\", \"labels\": \"uint32[]\"}\n",
        "\n",
        "with ParquetWriter(out=chini_out, columns=col, exist_ok=True) as w:\n",
        "    for row in ds:\n",
        "        tokens = tokenize(row[\"text\"])\n",
        "        w.write({\"input_ids\": tokens, \"labels\": tokens})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Read"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from chinidataset import StreamingDataset\n",
        "\n",
        "read_ds = StreamingDataset(local=chini_out)\n",
        "\n",
        "for sample in read_ds:\n",
        "    _ = sample[\"input_ids\"]"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
